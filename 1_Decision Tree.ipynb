{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "In this project, you will see how decision trees work by implementing a decision tree in sklearn.\n",
    "\n",
    "We'll start by loading the dataset and displaying some of its rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Survived</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>0</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import libraries necessary for this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display # Allows the use of display() for DataFrames\n",
    "\n",
    "# Pretty display for notebooks\n",
    "%matplotlib inline\n",
    "\n",
    "# Set a random seed\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# Load the dataset\n",
    "in_file = 'TitanicSurvival.csv'\n",
    "full_data = pd.read_csv(in_file)\n",
    "\n",
    "# Print the first few entries of the RMS Titanic data\n",
    "display(full_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Describe the features\n",
    "\n",
    "Recall that these are the various features present for each passenger on the ship:\n",
    "- **Survived**: Outcome of survival (0 = No, 1 = Yes)\n",
    "- **Pclass**: Passenger class (socio-economic status, 1 = 1st Upper, 2 = 2nd Middle, 3 = 3rd Lower)\n",
    "- **Name**: Full name of the passenger\n",
    "- **Sex**: Sex of the passenger (male or female)\n",
    "- **Age**: Age of the passenger\n",
    "- **SibSp**: Count of siblings and spouses of the passenger\n",
    "- **Parch**: Count of parents and children of the passenger\n",
    "- **Ticket**: Numbering system for identification of the passenger's ticket\n",
    "- **Fare**: Price paid in gbp by the passenger for the ticket\n",
    "- **Cabin**: Cabin number occupied by the passenger \n",
    "- **Embarked**: Port from which the passenger embarked on the ship\n",
    "\n",
    "Since we're interested in the outcome of survival for each passenger or crew member, we can remove the **Survived** feature from this dataset and store it as its own separate variable `outcomes`. We will use these outcomes as our prediction targets.  \n",
    "Run the code cell below to remove **Survived** as a feature of the dataset and store it in `outcomes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Name</th>\n      <th>Sex</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Ticket</th>\n      <th>Fare</th>\n      <th>Cabin</th>\n      <th>Embarked</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>3</td>\n      <td>Braund, Mr. Owen Harris</td>\n      <td>male</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>A/5 21171</td>\n      <td>7.2500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n      <td>female</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>PC 17599</td>\n      <td>71.2833</td>\n      <td>C85</td>\n      <td>C</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>3</td>\n      <td>Heikkinen, Miss. Laina</td>\n      <td>female</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>STON/O2. 3101282</td>\n      <td>7.9250</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n      <td>female</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113803</td>\n      <td>53.1000</td>\n      <td>C123</td>\n      <td>S</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>3</td>\n      <td>Allen, Mr. William Henry</td>\n      <td>male</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>373450</td>\n      <td>8.0500</td>\n      <td>NaN</td>\n      <td>S</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
      "text/plain": "   PassengerId  Pclass                                               Name  \\\n0            1       3                            Braund, Mr. Owen Harris   \n1            2       1  Cumings, Mrs. John Bradley (Florence Briggs Th...   \n2            3       3                             Heikkinen, Miss. Laina   \n3            4       1       Futrelle, Mrs. Jacques Heath (Lily May Peel)   \n4            5       3                           Allen, Mr. William Henry   \n\n      Sex   Age  SibSp  Parch            Ticket     Fare Cabin Embarked  \n0    male  22.0      1      0         A/5 21171   7.2500   NaN        S  \n1  female  38.0      1      0          PC 17599  71.2833   C85        C  \n2  female  26.0      0      0  STON/O2. 3101282   7.9250   NaN        S  \n3  female  35.0      1      0            113803  53.1000  C123        S  \n4    male  35.0      0      0            373450   8.0500   NaN        S  "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Store the 'Survived' feature in a new variable\n",
    "outcomes = full_data['Survived']\n",
    "\n",
    "#  Remove it from the dataset\n",
    "keep_col = ['PassengerId', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp', 'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n",
    "features_raw = full_data[keep_col]\n",
    "\n",
    "# Show the new dataset with 'Survived' removed\n",
    "display(features_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The very same sample of the RMS Titanic data now shows the **Survived** feature removed from the DataFrame. Note that `data` (the passenger data) and `outcomes` (the outcomes of survival) are now *paired*. That means for any passenger `data.loc[i]`, they have the survival outcome `outcomes[i]`.\n",
    "\n",
    "## (TODO) Preprocessing the data\n",
    "\n",
    "Now, let's do some data preprocessing. First, we'll one-hot encode the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-Hot encoding for all our categorical variables\n",
    "features = pd.get_dummies(features_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we'll fill in any blanks with zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>Pclass</th>\n      <th>Age</th>\n      <th>SibSp</th>\n      <th>Parch</th>\n      <th>Fare</th>\n      <th>Name_Abbing, Mr. Anthony</th>\n      <th>Name_Abbott, Mr. Rossmore Edward</th>\n      <th>Name_Abbott, Mrs. Stanton (Rosa Hunt)</th>\n      <th>Name_Abelson, Mr. Samuel</th>\n      <th>...</th>\n      <th>Cabin_F G73</th>\n      <th>Cabin_F2</th>\n      <th>Cabin_F33</th>\n      <th>Cabin_F38</th>\n      <th>Cabin_F4</th>\n      <th>Cabin_G6</th>\n      <th>Cabin_T</th>\n      <th>Embarked_C</th>\n      <th>Embarked_Q</th>\n      <th>Embarked_S</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>3</td>\n      <td>22.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>7.2500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n      <td>38.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>71.2833</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>3</td>\n      <td>26.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7.9250</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>1</td>\n      <td>35.0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>53.1000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>3</td>\n      <td>35.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>8.0500</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1730 columns</p>\n</div>",
      "text/plain": "   PassengerId  Pclass   Age  SibSp  Parch     Fare  Name_Abbing, Mr. Anthony  \\\n0            1       3  22.0      1      0   7.2500                         0   \n1            2       1  38.0      1      0  71.2833                         0   \n2            3       3  26.0      0      0   7.9250                         0   \n3            4       1  35.0      1      0  53.1000                         0   \n4            5       3  35.0      0      0   8.0500                         0   \n\n   Name_Abbott, Mr. Rossmore Edward  Name_Abbott, Mrs. Stanton (Rosa Hunt)  \\\n0                                 0                                      0   \n1                                 0                                      0   \n2                                 0                                      0   \n3                                 0                                      0   \n4                                 0                                      0   \n\n   Name_Abelson, Mr. Samuel  ...  Cabin_F G73  Cabin_F2  Cabin_F33  Cabin_F38  \\\n0                         0  ...            0         0          0          0   \n1                         0  ...            0         0          0          0   \n2                         0  ...            0         0          0          0   \n3                         0  ...            0         0          0          0   \n4                         0  ...            0         0          0          0   \n\n   Cabin_F4  Cabin_G6  Cabin_T  Embarked_C  Embarked_Q  Embarked_S  \n0         0         0        0           0           0           1  \n1         0         0        0           1           0           0  \n2         0         0        0           0           0           1  \n3         0         0        0           0           0           1  \n4         0         0        0           0           0           1  \n\n[5 rows x 1730 columns]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features = features.fillna(0)\n",
    "display(features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Training the model\n",
    "\n",
    "Now we're ready to train a model in sklearn. First, let's split the data into training and testing sets (80%-20%). Then we'll train the model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, outcomes, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=None, splitter='best')"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the classifier from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# TODO: Define the classifier, and fit it to the data\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO) Testing the model\n",
    "Now, let's see how our model does, let's calculate the accuracy over both the training and the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "The training accuracy is 1.0\nThe test accuracy is 0.8044692737430168\n"
    }
   ],
   "source": [
    "# Making predictions\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('The training accuracy is', train_accuracy)\n",
    "print('The test accuracy is', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO)  Improving the model\n",
    "\n",
    "Ok, high training accuracy and a lower testing accuracy. We may be overfitting a bit.\n",
    "\n",
    "So now it's your turn to shine! Train a new model, and try to specify some parameters in order to improve the testing accuracy, such as:\n",
    "- `max_depth`\n",
    "- `min_samples_leaf`\n",
    "- `min_samples_split`\n",
    "\n",
    "You can use your intuition, trial and error, or even better, feel free to use Grid Search!\n",
    "\n",
    "**Challenge:** Try to get to 85% accuracy on the testing set. If you'd like a hint, take a look at the solutions notebook next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "BestCLF DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=8,\n                       max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=4, min_samples_split=14,\n                       min_weight_fraction_leaf=0.0, presort=False,\n                       random_state=None, splitter='best')\nBestPar {'max_depth': 8, 'min_samples_leaf': 4, 'min_samples_split': 14}\nThe training accuracy is 0.8820224719101124\nThe test accuracy is 0.8547486033519553\n"
    }
   ],
   "source": [
    "# Importing metrics and Grid Search\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Choosing the parameters list\n",
    "sample_split_range = list(range(2, 15, 2))\n",
    "parameters = {'max_depth':sample_split_range,'min_samples_leaf':sample_split_range, 'min_samples_split':sample_split_range}\n",
    "\n",
    "# Grid Search\n",
    "grid = GridSearchCV(model, parameters, cv = 5, scoring = 'accuracy')\n",
    "\n",
    "# Fitting\n",
    "grid_fit = grid.fit(X_train, y_train)\n",
    "\n",
    "# Finding the best estimator\n",
    "best_clf = grid_fit.best_estimator_\n",
    "print('BestCLF', best_clf)\n",
    "best_par =grid_fit.best_params_\n",
    "print('BestPar', best_par)\n",
    "\n",
    "# TODO: Train the model\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# TODO: Make predictions\n",
    "y_train_pred = best_clf.predict(X_train)\n",
    "y_test_pred = best_clf.predict(X_test)\n",
    "\n",
    "# TODO: Calculate the accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print('The training accuracy is', train_accuracy)\n",
    "print('The test accuracy is', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "1) Describe one real-world application in industry where the model can be applied.  \n",
    "2) What are the strengths of the model; when does it perform well?  \n",
    "3) What are the weaknesses of the model; when does it perform poorly?  \n",
    "4) What makes this model a good candidate for the problem, given what you know about the data?  \n",
    "  \n",
    "Please include references with your answer.  \n",
    "\n",
    "#### Answer\n",
    "1) Applications of Decision Tree Machine Learning Algorithm\n",
    "\n",
    "- Decision trees are among the popular machine learning algorithms that find great use in finance for option pricing.\n",
    "- Remote sensing is an application area for pattern recognition based on decision trees.\n",
    "- Decision tree algorithms are used by banks to classify loan applicants by their probability of defaulting payments.\n",
    "- Gerber Products, a popular baby product company, used decision tree machine learning algorithm to decide whether they should continue using the plastic PVC (Poly Vinyl Chloride) in their products.\n",
    "- Rush University Medical Centre has developed a tool named Guardian that uses a decision tree machine learning algorithm to identify at-risk patients and disease trends.\n",
    "   \n",
    "2) Advantages of Using Decision Tree Machine Learning Algorithms\n",
    "\n",
    "- Decision trees are very instinctual and can be explained to anyone with ease. People from a non-technical background, can also decipher the hypothesis drawn from a decision tree, as they are self-explanatory. A decision tree is simple to understand and once it is understood, we can construct it.    \n",
    "- When using decision tree machine learning algorithms, data type is not a constraint as they can handle both categorical and numerical variables.  \n",
    "- Decision tree machine learning algorithms do not require making any assumption on the linearity in the data and hence can be used in circumstances where the parameters are non-linearly related. These machine learning algorithms do not make any assumptions on the classifier structure and space distribution.  \n",
    "- These algorithms are useful in data exploration. Decision trees implicitly perform feature selection which is very important in predictive analytics. When a decision tree is fit to a training dataset, the nodes at the top on which the decision tree is split, are considered as important variables within a given dataset and feature selection is completed by default.  \n",
    "- They are also time-efficient with large data. Decision trees help save data preparation time, as they are not sensitive to missing values and outliers. Missing values will not stop you from splitting the data for building a decision tree. Outliers will also not affect the decision trees as data splitting happens based on some samples within the split range and not on exact absolute values.  \n",
    "- It requires less effort for the training of the data.  \n",
    "- Decision Tree is proven to be a robust model with promising outcomes.  \n",
    "   \n",
    "3) Drawbacks of Using Decision Tree Machine Learning Algorithms  \n",
    "\n",
    "- The more the number of decisions in a tree, less is the accuracy of any expected outcome.  \n",
    "- A major drawback of decision tree machine learning algorithms, is that the outcomes may be based on expectations. When decisions are made in real-time, the payoffs and resulting outcomes might not be the same as expected or planned. There are chances that this could lead to unrealistic decision trees leading to bad decision making. Any irrational expectations could lead to major errors and flaws in decision tree analysis, as it is not always possible to plan for all eventualities that can arise from a decision.  \n",
    "- Decision Trees do not fit well for continuous variables and result in instability and classification plateaus. Only if the information is precise and accurate, the decision tree will deliver promising results. Even if there is a slight change in the input data, it can cause large changes in the tree.  \n",
    "- Decision trees are easy to use when compared to other decision making models but creating large decision trees that contain several branches is a complex and time consuming task.  \n",
    "- Decision tree machine learning algorithms consider only one attribute at a time and might not be best suited for actual data in the decision space.  \n",
    "- Large sized decision trees with multiple branches are not comprehensible and pose several presentation difficulties.  \n",
    "- Costs. Sometimes cost also remains a main factor because when one is required to construct a complex decision tree, it requires advanced knowledge in quantitative and statistical analysis.  \n",
    "  \n",
    "4) Decision tree algorithm is a good candidate for the titanic survival problem:  \n",
    "- They can classify attributes which are categorical variables.  \n",
    "- They can handle missing values nicely by looking at the data in other columns.  \n",
    "- They are quite efficient in dealing with the target function that has discrete output values.  \n",
    "- Some of the instances ('sex', 'embarked') are represented by attribute value pairs\n",
    "\n",
    "References:  \n",
    "https://www.dezyre.com/article/top-10-machine-learning-algorithms/202  \n",
    "https://www.educba.com/decision-tree-algorithm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (TODO)  Describe your conclusions  \n",
    "\n",
    "In this notebook we learned how to use Desicion Tree algorithm in order to classify our data. Pandas library helped us read, understand, preprocess and clean our data, which were in a csv file format. The data was about Titanic survival, so the model we had to implement was if an X passenger survived given our features. Then we trained and tested our Desicion Tree model using Scikit-learn libary and after that we improved its accuracy with Grid search. Finally we dived into the Desicion Tree algorithm answering the next questions. Where is it used, what are its advantages and disadvantages and why we could use it in our problem knowing our data?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}